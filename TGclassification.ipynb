{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Given the data set, do a quick exploratory data analysis to get a feel for the distributions and biases of the data.  Report any visualizations and findings used and suggest any other impactful business use cases for that data.\n",
    " Looking at this dataset we have 5 different collumns, Year, Major, University, Time, Order.\n",
    " It looks like places have preferences on what they get or how the food is seasoned, this is important to think about as we would want to try and pander to these preferences to achieve the highest sales for each location.\n",
    " Just looking at the data I dont see much correlation between what major the individual is pursuing and their order but a visualtion of the data through a graph would be much more helpful here than just looking at it. \n",
    " Time can be a major factor as it appears that most orders are done between lunch time (11-13) and a few for dinner (14-18) and even less for breakfast (8-11), so it is important that we monitor when people are most likely to be at the\n",
    "   food truck as to limit wasted employe time and our money. \n",
    " There looks like there might be a correlation between year and the time when the customer orders their food but it doesn't appear to be a major correlation when just viewing the data with my eyes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider implications of data collection, storage, and data biases you would consider relevant here considering Data Ethics, Business Outcomes, and Technical Implications\n",
    "1: Discuss Ethical implications of these factors\n",
    "    With data collection it is our duty that we are able to collect only data that is needed for our operations. We shouldn't be collecting private intimate details of our customers unless it is needed for us to acheive our goals as a company.\n",
    "        We need to be careful with our stoarge of client data, hiding it and securing it as to prevent any eyes but our own from viewing it. A company needs the customers trust for continued survival and as such we have to do everything in our power to protect our clients information at every stage of collection and when we store it ourselves. We also have to refrain from biases with collection of our data, we must handle all data without any preference of the collector. We cannot handpick data from the datasets, it must be looked at as it is and our models must reflect the real data to provide accurate results. We must also balance all of this with the success of our company as we need this data to remian competive and to keep the company alive, but we cannot hurt the customer who we collect the data from.\n",
    "2: Discuss Business outcome implications of these factors\n",
    "    It is important that we consider every step we make as a company to remain unbiased in our collection and use of the data that we collect. We cannot allow an individual to have influence on the datasets that we collect or use as it will impact the service quality we can provide for our clients. If our datacollection is not handled correctly it can mean serious consequences and repercussions for our company. We must do our best to handle all aspects of data collection, stoarge and data biases as to protect the company while providing strong service. \n",
    "3: Discuss Technical Implications\n",
    "    With data collection it is important we collect only the data we need. Unless it is absolutely needed for our operations we shouldn't hold private information of our clients. Here with this dataset it was handled well, we only have very broad data which prevents hostile entites from using this data in malicious ways. We can enchance the quality of our stoarge through encryption or obfuscation, by using code word or abbreviations or by hiding everything in a private format we can further limit the potential damage if this data were to ever leave our hands. Data biases pose a huge threat towards the practal use of the data we collect. If our data is being used to predict the future outcomes of clients who use our service it is important that we know their true actions and not ones cherry picked by us. Failure here would result in a predictive model that would be almost completly useless when deployed in the real world."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Year                    Major                University  Time  \\\n",
      "0     Year 2                  Physics  Indiana State University    12   \n",
      "1     Year 3                Chemistry     Ball State University    14   \n",
      "2     Year 3                Chemistry         Butler University    12   \n",
      "3     Year 2                  Biology  Indiana State University    11   \n",
      "4     Year 3  Business Administration         Butler University    12   \n",
      "...      ...                      ...                       ...   ...   \n",
      "4995  Year 2              Mathematics     Ball State University    11   \n",
      "4996  Year 3                Astronomy  Indiana State University    12   \n",
      "4997  Year 3                Chemistry         Butler University    13   \n",
      "4998  Year 3                Astronomy         Butler University    15   \n",
      "4999  Year 2                Economics  University of Notre Dame    15   \n",
      "\n",
      "                                                  Order  \n",
      "0                                  Fried Catfish Basket  \n",
      "1                                       Sugar Cream Pie  \n",
      "2                                    Indiana Pork Chili  \n",
      "3                                  Fried Catfish Basket  \n",
      "4     Indiana Corn on the Cob (brushed with garlic b...  \n",
      "...                                                 ...  \n",
      "4995                   Breaded Pork Tenderloin Sandwich  \n",
      "4996  Ultimate Grilled Cheese Sandwich (with bacon a...  \n",
      "4997                                    Sugar Cream Pie  \n",
      "4998                                    Sugar Cream Pie  \n",
      "4999                   Breaded Pork Tenderloin Sandwich  \n",
      "\n",
      "[5000 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "#Build a model to predict a customers order from their available information. \n",
    "\n",
    "#Because we are trying to perdict what a person would pick to eat based on who they are I think we should be focused on a classification model\n",
    "#For training I will be using the first 80% of the data which is roughly the first 4000 entries\n",
    "#I will then test the model on the remaining 20% of the data which will be 1001 entries of the data\n",
    "#Normally to prep data we would do the obvious things first such as removing duplicates, outliers or areas where we have NULL values in our data.\n",
    "#I will go ahead and run the data through to clean out any null values but I didn't see any when I was looking through.\n",
    "# With this dataset I dont think we should remove duplicates as it is entirely possible someone of the same year,major,location and time ordered the exact same thing. Maybe they had a party or something, so we shouldn't remove that here\n",
    "    #unless we store quantites of when this happened but the data set doesn't have that.\n",
    "#I will go through and find the unique values of all the majors, locations and orders so I know what the possibilties are of who a person can be and any possible orders.\n",
    "#I will also go ahead and switch everything to lowercase, remove spaces and remove text in parentheses as it isn't needed for us here.\n",
    "\n",
    "#I think we can \n",
    "\n",
    "#Import skLearn, numpy and panda\n",
    "import sklearn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "#First we will import the dataset in so we can start working on it\n",
    "df = pd.read_excel(r\"C:\\Users\\trist\\OneDrive\\Documents\\XternInternship\\xtern.xlsx\")\n",
    "#Check the dataframe to make sure it imported correctly\n",
    "print(df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Year                   Major              University  Time  \\\n",
      "0     year2                 physics  indianastateuniversity    12   \n",
      "1     year3               chemistry     ballstateuniversity    14   \n",
      "2     year3               chemistry        butleruniversity    12   \n",
      "3     year2                 biology  indianastateuniversity    11   \n",
      "4     year3  businessadministration        butleruniversity    12   \n",
      "...     ...                     ...                     ...   ...   \n",
      "4995  year2             mathematics     ballstateuniversity    11   \n",
      "4996  year3               astronomy  indianastateuniversity    12   \n",
      "4997  year3               chemistry        butleruniversity    13   \n",
      "4998  year3               astronomy        butleruniversity    15   \n",
      "4999  year2               economics   universityofnotredame    15   \n",
      "\n",
      "                                                Order  \n",
      "0                                  friedcatfishbasket  \n",
      "1                                       sugarcreampie  \n",
      "2                                    indianaporkchili  \n",
      "3                                  friedcatfishbasket  \n",
      "4          indianacornonthecobbrushedwithgarlicbutter  \n",
      "...                                               ...  \n",
      "4995                    breadedporktenderloinsandwich  \n",
      "4996  ultimategrilledcheesesandwichwithbaconandtomato  \n",
      "4997                                    sugarcreampie  \n",
      "4998                                    sugarcreampie  \n",
      "4999                    breadedporktenderloinsandwich  \n",
      "\n",
      "[5000 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "#Lets start cleaning the data :)\n",
    "#Remove any special character from year column\n",
    "df['Year'] = df['Year'].str.replace(r'\\W',\"\")\n",
    "\n",
    "#Remove all spaces from year\n",
    "df['Year'] = df[\"Year\"].str.replace(\" \", \"\")\n",
    "\n",
    "#lets make year all lowercase as well\n",
    "df['Year'] = df['Year'].str.lower()\n",
    "\n",
    "#Now we will repeat this process for the rest of the columns\n",
    "\n",
    "#Major\n",
    "#Remove any special character from Major column\n",
    "df['Major'] = df['Major'].str.replace(r'\\W',\"\")\n",
    "\n",
    "#Remove all spaces from Major\n",
    "df['Major'] = df[\"Major\"].str.replace(\" \", \"\")\n",
    "\n",
    "#lets make Major all lowercase as well\n",
    "df['Major'] = df['Major'].str.lower()\n",
    "\n",
    "#University or Location\n",
    "#Remove any special character from University column\n",
    "df['University'] = df['University'].str.replace(r'\\W',\"\")\n",
    "\n",
    "#Remove all spaces from University\n",
    "df['University'] = df[\"University\"].str.replace(\" \", \"\")\n",
    "\n",
    "#lets make University all lowercase as well\n",
    "df['University'] = df['University'].str.lower()\n",
    "\n",
    "#Time\n",
    "#Because time is numeric we don't need to touch it here\n",
    "#I am very happy to see this :)\n",
    "\n",
    "#Order\n",
    "#Here I also want to remove the parenthesis as well, I don't like having them in the data\n",
    "#Remove any special character from Order column\n",
    "df['Order'] = df['Order'].str.replace(r'\\W',\"\")\n",
    "\n",
    "#Remove left parenthesis\n",
    "df['Order'] = df['Order'].str.replace(\"(\", \"\")\n",
    "\n",
    "#Remove right parenthesis\n",
    "df['Order'] = df['Order'].str.replace(\")\", \"\")\n",
    "#I could have used the str.sub here instead but replace works just fine!\n",
    "\n",
    "#Remove all spaces from Order\n",
    "df['Order'] = df[\"Order\"].str.replace(\" \", \"\")\n",
    "\n",
    "#lets make Order all lowercase as well\n",
    "df['Order'] = df['Order'].str.lower()\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "DataFrame constructor not properly called!",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_6604\\2214427305.py\u001b[0m in \u001b[0;36m?\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m#Convert back to dataframe format\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[0;32m    840\u001b[0m                 )\n\u001b[0;32m    841\u001b[0m         \u001b[1;31m# For data is scalar\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    842\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    843\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mindex\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mcolumns\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 844\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"DataFrame constructor not properly called!\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    845\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    846\u001b[0m             \u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mensure_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    847\u001b[0m             \u001b[0mcolumns\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mensure_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: DataFrame constructor not properly called!"
     ]
    }
   ],
   "source": [
    "#Now I will convert the excel file into a CSV to help with further work\n",
    "\n",
    "df = df.to_csv\n",
    "\n",
    "#Convert back to dataframe format\n",
    "df = pd.DataFrame(df)\n",
    "print(df)\n",
    "type(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'function' object has no attribute 'loc'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\trist\\OneDrive\\Documents\\XternInternship\\TGclassification.ipynb Cell 6\u001b[0m line \u001b[0;36m1\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/trist/OneDrive/Documents/XternInternship/TGclassification.ipynb#W5sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mfeature_extraction\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtext\u001b[39;00m \u001b[39mimport\u001b[39;00m TfidfTransformer\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/trist/OneDrive/Documents/XternInternship/TGclassification.ipynb#W5sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39m#First we get two things to compare to\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/trist/OneDrive/Documents/XternInternship/TGclassification.ipynb#W5sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m x \u001b[39m=\u001b[39m df\u001b[39m.\u001b[39;49mloc[:, \u001b[39m'\u001b[39m\u001b[39mUniversity\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/trist/OneDrive/Documents/XternInternship/TGclassification.ipynb#W5sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m y \u001b[39m=\u001b[39m df\u001b[39m.\u001b[39mloc[\u001b[39m\"\u001b[39m\u001b[39mOrder\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/trist/OneDrive/Documents/XternInternship/TGclassification.ipynb#W5sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m \u001b[39m#Now we split up the dataset with trainings and testings for both features\u001b[39;00m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'function' object has no attribute 'loc'"
     ]
    }
   ],
   "source": [
    "#Lets start creating a model\n",
    "#I will be making a logistic regression model\n",
    "\n",
    "#import model and pipeline and Tf-idf\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "#First we get two things to compare to\n",
    "x = df.loc[:, 'University']\n",
    "y = df.loc[\"Order\"]\n",
    "\n",
    "#Now we split up the dataset with trainings and testings for both features\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state = 30)\n",
    "\n",
    "#This model will be comparing loaction to order\n",
    "#Using pipeline from sckit learn to combine data\n",
    "#We will convert the text to a vector than to a tf-idf for frequency\n",
    "logesticRegression = Pipeline([('vect', CountVectorizer()), ('tfidf', TfidfTransformer()), ('clf', LogisticRegression(n_jobs=1, C=1e5)),])\n",
    "\n",
    "#Fit the logesticRegression to our data\n",
    "logesticRegression.fit(x_train, y_train)\n",
    "\n",
    "y_pred = logreg.predict(X_test)\n",
    "\n",
    "print('accuracy %s' % accuracy_score(y_pred, y_test))\n",
    "print(classification_report(y_test, y_pred,target_names=my_tags))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
