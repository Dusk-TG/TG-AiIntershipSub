{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the data set, do a quick exploratory data analysis to get a feel for the distributions and biases of the data.  Report any visualizations and findings used and suggest any other impactful business use cases for that data.\n",
    " Looking at this dataset we have 5 different columns, Year, Major, University, Time, Order.\n",
    " It looks like places have preferences on what they get or how the food is seasoned, this is important to think about as we would want to try and pander to these preferences to achieve the highest sales for each location.\n",
    " Just looking at the data I dont see much correlation between what major the individual is pursuing and their order but a visualization of the data through a graph would be much more helpful here than just looking at it. \n",
    " Time can be a major factor as it appears that most orders are done between lunch time (11-13) and a few for dinner (14-18) and even less for breakfast (8-11), so it is important that we monitor when people are most likely to be at the\n",
    "   food truck as to limit wasted employe time and our money. \n",
    " There looks like there might be a correlation between the year and the time when the customer orders their food, but it doesn't appear to be a major correlation when just viewing the data with my eyes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider implications of data collection, storage, and data biases you would consider relevant here considering Data Ethics, Business Outcomes, and Technical Implications\n",
    "1: Discuss Ethical implications of these factors\n",
    "    With data collection it is our duty that we can collect only data that is needed for our operations. We shouldn't be collecting private intimate details of our customers unless it is needed for us to achieve our goals as a company.\n",
    "        We need to be careful with our storage of client data, hiding it and securing it to prevent any eyes but our own from viewing it. A company needs the customers’ trust for continued survival and as such we must do everything in our power to protect our client’s information at every stage of collection and when we store it ourselves. We also must refrain from biases with collection of our data, we must handle all data without any preference of the collector. We cannot handpick data from the datasets, it must be looked at as it is, and our models must reflect the real data to provide accurate results. We must also balance all of this with the success of our company as we need this data to remain competitive and to keep the company alive, but we cannot hurt the customer who we collect the data from.\n",
    "2: Discuss Business outcome implications of these factors\n",
    "    It is important that we consider every step we take as a company to remain unbiased in our collection and use of the data that we collect. We cannot allow an individual to have an influence on the datasets that we collect or use as it will impact the service quality we can provide for our clients. If our data collection is not handled correctly, it can have serious consequences and repercussions for our company. We must do our best to handle all aspects of data collection, storage and data biases to protect the company while providing a strong service. \n",
    "3: Discuss Technical Implications\n",
    "    With data collection it is important we collect only the data we need. Unless it is absolutely needed for our operations, we shouldn't hold the private information of our clients. Here with this dataset, it was handled well, we only have very broad data which prevents hostile entities from using this data in malicious ways. We can enhance the quality of our storage through encryption or obfuscation, by using code word or abbreviations or by hiding everything in a private format. We can further limit the potential damage if this data were to ever leave our hands. Data biases pose a huge threat to the practical use of the data we collect. If our data is being used to predict the future outcomes of clients who use our service, it is important that we know their true actions and not one’s cherry picked by us. Failure here would result in a predictive model that would be almost completely useless when deployed in the real world."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Year                    Major                University  Time  \\\n",
      "0     Year 2                  Physics  Indiana State University    12   \n",
      "1     Year 3                Chemistry     Ball State University    14   \n",
      "2     Year 3                Chemistry         Butler University    12   \n",
      "3     Year 2                  Biology  Indiana State University    11   \n",
      "4     Year 3  Business Administration         Butler University    12   \n",
      "...      ...                      ...                       ...   ...   \n",
      "4995  Year 2              Mathematics     Ball State University    11   \n",
      "4996  Year 3                Astronomy  Indiana State University    12   \n",
      "4997  Year 3                Chemistry         Butler University    13   \n",
      "4998  Year 3                Astronomy         Butler University    15   \n",
      "4999  Year 2                Economics  University of Notre Dame    15   \n",
      "\n",
      "                                                  Order  \n",
      "0                                  Fried Catfish Basket  \n",
      "1                                       Sugar Cream Pie  \n",
      "2                                    Indiana Pork Chili  \n",
      "3                                  Fried Catfish Basket  \n",
      "4     Indiana Corn on the Cob (brushed with garlic b...  \n",
      "...                                                 ...  \n",
      "4995                   Breaded Pork Tenderloin Sandwich  \n",
      "4996  Ultimate Grilled Cheese Sandwich (with bacon a...  \n",
      "4997                                    Sugar Cream Pie  \n",
      "4998                                    Sugar Cream Pie  \n",
      "4999                   Breaded Pork Tenderloin Sandwich  \n",
      "\n",
      "[5000 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "#Build a model to predict a customers order from their available information. \n",
    "\n",
    "#Because we are trying to perdict what a person would pick to eat based on who they are I think we should be focused on a classification model\n",
    "#For training I will be using the first 80% of the data which is roughly the first 4000 entries\n",
    "#I will then test the model on the remaining 20% of the data which will be 1001 entries of the data\n",
    "#Normally to prep data we would do the obvious things first such as removing duplicates, outliers or areas where we have NULL values in our data.\n",
    "#I will go ahead and run the data through to clean out any null values but I didn't see any when I was looking through.\n",
    "# With this dataset I dont think we should remove duplicates as it is entirely possible someone of the same year,major,location and time ordered the exact same thing. Maybe they had a party or something, so we shouldn't remove that here\n",
    "    #unless we store quantites of when this happened but the data set doesn't have that.\n",
    "#I will go through and find the unique values of all the majors, locations and orders so I know what the possibilties are of who a person can be and any possible orders.\n",
    "#I will also go ahead and switch everything to lowercase, remove spaces and remove text in parentheses as it isn't needed for us here.\n",
    "\n",
    "#I think we can \n",
    "\n",
    "#Import skLearn, numpy and panda\n",
    "import sklearn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "#First we will import the dataset in so we can start working on it\n",
    "df = pd.read_excel(r\"C:\\Users\\trist\\OneDrive\\Documents\\XternInternship\\xtern.xlsx\")\n",
    "#Check the dataframe to make sure it imported correctly\n",
    "print(df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Indiana State University' 'Ball State University' 'Butler University'\n",
      " 'Indiana University-Purdue University Indianapolis (IUPUI)'\n",
      " 'University of Notre Dame' 'University of Evansville'\n",
      " 'Valparaiso University' 'Purdue University'\n",
      " 'Indiana University Bloomington' 'DePauw University']\n",
      "       Year                   Major              University  Time  \\\n",
      "0     year2                 physics  indianastateuniversity    12   \n",
      "1     year3               chemistry     ballstateuniversity    14   \n",
      "2     year3               chemistry        butleruniversity    12   \n",
      "3     year2                 biology  indianastateuniversity    11   \n",
      "4     year3  businessadministration        butleruniversity    12   \n",
      "...     ...                     ...                     ...   ...   \n",
      "4995  year2             mathematics     ballstateuniversity    11   \n",
      "4996  year3               astronomy  indianastateuniversity    12   \n",
      "4997  year3               chemistry        butleruniversity    13   \n",
      "4998  year3               astronomy        butleruniversity    15   \n",
      "4999  year2               economics   universityofnotredame    15   \n",
      "\n",
      "                                                Order  \n",
      "0                                  friedcatfishbasket  \n",
      "1                                       sugarcreampie  \n",
      "2                                    indianaporkchili  \n",
      "3                                  friedcatfishbasket  \n",
      "4          indianacornonthecobbrushedwithgarlicbutter  \n",
      "...                                               ...  \n",
      "4995                    breadedporktenderloinsandwich  \n",
      "4996  ultimategrilledcheesesandwichwithbaconandtomato  \n",
      "4997                                    sugarcreampie  \n",
      "4998                                    sugarcreampie  \n",
      "4999                    breadedporktenderloinsandwich  \n",
      "\n",
      "[5000 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "#Lets start cleaning the data :)\n",
    "\n",
    "#I am going to make a list here before cleaning for unique values of each of the columns for lists in the future\n",
    "UniNames = df[\"University\"].unique()\n",
    "Years = df[\"Year\"].unique()\n",
    "Majors = df[\"Major\"].unique()\n",
    "Times = df[\"Time\"].unique()\n",
    "Orders = df[\"Order\"].unique()\n",
    "print(UniNames)\n",
    "\n",
    "#Remove any special character from year column\n",
    "df['Year'] = df['Year'].str.replace(r'\\W',\"\")\n",
    "\n",
    "#Remove all spaces from year\n",
    "df['Year'] = df[\"Year\"].str.replace(\" \", \"\")\n",
    "\n",
    "#lets make year all lowercase as well\n",
    "df['Year'] = df['Year'].str.lower()\n",
    "\n",
    "#Now we will repeat this process for the rest of the columns\n",
    "\n",
    "#Major\n",
    "#Remove any special character from Major column\n",
    "df['Major'] = df['Major'].str.replace(r'\\W',\"\")\n",
    "\n",
    "#Remove all spaces from Major\n",
    "df['Major'] = df[\"Major\"].str.replace(\" \", \"\")\n",
    "\n",
    "#lets make Major all lowercase as well\n",
    "df['Major'] = df['Major'].str.lower()\n",
    "\n",
    "#University or Location\n",
    "#Remove any special character from University column\n",
    "df['University'] = df['University'].str.replace(r'\\W',\"\")\n",
    "\n",
    "#Remove all spaces from University\n",
    "df['University'] = df[\"University\"].str.replace(\" \", \"\")\n",
    "\n",
    "#lets make University all lowercase as well\n",
    "df['University'] = df['University'].str.lower()\n",
    "\n",
    "#Time\n",
    "#Because time is numeric we don't need to touch it here\n",
    "#I am very happy to see this :)\n",
    "\n",
    "#Order\n",
    "#Here I also want to remove the parenthesis as well, I don't like having them in the data\n",
    "#Remove any special character from Order column\n",
    "df['Order'] = df['Order'].str.replace(r'\\W',\"\")\n",
    "\n",
    "#Remove left parenthesis\n",
    "df['Order'] = df['Order'].str.replace(\"(\", \"\")\n",
    "\n",
    "#Remove right parenthesis\n",
    "df['Order'] = df['Order'].str.replace(\")\", \"\")\n",
    "#I could have used the str.sub here instead but replace works just fine!\n",
    "\n",
    "#Remove all spaces from Order\n",
    "df['Order'] = df[\"Order\"].str.replace(\" \", \"\")\n",
    "\n",
    "#lets make Order all lowercase as well\n",
    "df['Order'] = df['Order'].str.lower()\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\trist\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\trist\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\trist\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\trist\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\trist\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "University and Order\n",
      "accuracy 0.19\n",
      "                                                          precision    recall  f1-score   support\n",
      "\n",
      "                                    Fried Catfish Basket       0.00      0.00      0.00        96\n",
      "                                         Sugar Cream Pie       0.00      0.00      0.00       100\n",
      "                                      Indiana Pork Chili       0.00      0.00      0.00       100\n",
      "    Indiana Corn on the Cob (brushed with garlic butter)       0.21      0.55      0.30        95\n",
      "                 Indiana Buffalo Chicken Tacos (3 tacos)       0.18      0.27      0.22        99\n",
      "                                      Sweet Potato Fries       0.20      0.44      0.28       101\n",
      "Ultimate Grilled Cheese Sandwich (with bacon and tomato)       0.16      0.56      0.25        94\n",
      "                        Breaded Pork Tenderloin Sandwich       0.00      0.00      0.00       112\n",
      "                                  Cornbread Hush Puppies       0.16      0.05      0.07       109\n",
      "                        Hoosier BBQ Pulled Pork Sandwich       0.39      0.10      0.15        94\n",
      "\n",
      "                                                accuracy                           0.19      1000\n",
      "                                               macro avg       0.13      0.20      0.13      1000\n",
      "                                            weighted avg       0.13      0.19      0.12      1000\n",
      "\n",
      "Major and Order\n",
      "accuracy 0.223\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\trist\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\trist\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\trist\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\trist\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\trist\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                          precision    recall  f1-score   support\n",
      "\n",
      "                                    Fried Catfish Basket       0.27      0.12      0.17        96\n",
      "                                         Sugar Cream Pie       0.00      0.00      0.00       100\n",
      "                                      Indiana Pork Chili       0.31      0.18      0.23       100\n",
      "    Indiana Corn on the Cob (brushed with garlic butter)       0.00      0.00      0.00        95\n",
      "                 Indiana Buffalo Chicken Tacos (3 tacos)       0.31      0.12      0.17        99\n",
      "                                      Sweet Potato Fries       0.00      0.00      0.00       101\n",
      "Ultimate Grilled Cheese Sandwich (with bacon and tomato)       0.21      0.80      0.33        94\n",
      "                        Breaded Pork Tenderloin Sandwich       0.33      0.18      0.23       112\n",
      "                                  Cornbread Hush Puppies       0.00      0.00      0.00       109\n",
      "                        Hoosier BBQ Pulled Pork Sandwich       0.20      0.91      0.32        94\n",
      "\n",
      "                                                accuracy                           0.22      1000\n",
      "                                               macro avg       0.16      0.23      0.15      1000\n",
      "                                            weighted avg       0.16      0.22      0.14      1000\n",
      "\n",
      "Year and Order\n",
      "accuracy 0.163\n",
      "                                                          precision    recall  f1-score   support\n",
      "\n",
      "                                    Fried Catfish Basket       0.00      0.00      0.00        96\n",
      "                                         Sugar Cream Pie       0.16      0.90      0.28       100\n",
      "                                      Indiana Pork Chili       0.00      0.00      0.00       100\n",
      "    Indiana Corn on the Cob (brushed with garlic butter)       0.00      0.00      0.00        95\n",
      "                 Indiana Buffalo Chicken Tacos (3 tacos)       0.00      0.00      0.00        99\n",
      "                                      Sweet Potato Fries       0.00      0.00      0.00       101\n",
      "Ultimate Grilled Cheese Sandwich (with bacon and tomato)       0.16      0.78      0.27        94\n",
      "                        Breaded Pork Tenderloin Sandwich       0.00      0.00      0.00       112\n",
      "                                  Cornbread Hush Puppies       0.00      0.00      0.00       109\n",
      "                        Hoosier BBQ Pulled Pork Sandwich       0.00      0.00      0.00        94\n",
      "\n",
      "                                                accuracy                           0.16      1000\n",
      "                                               macro avg       0.03      0.17      0.05      1000\n",
      "                                            weighted avg       0.03      0.16      0.05      1000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\trist\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "#Lets start creating a model function\n",
    "#I will be making a logistic regression model\n",
    "#It might not be the best model for this but I think it will be fun to try!\n",
    "\n",
    "def logesticModelerFun(nameOfX, nameOfY):\n",
    "    #import model and pipeline and Tf-idf and all the other fun goodies from sklearn\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    from sklearn.pipeline import Pipeline\n",
    "    from sklearn.feature_extraction.text import TfidfTransformer\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.feature_extraction.text import CountVectorizer\n",
    "    from sklearn.metrics import accuracy_score\n",
    "    from sklearn.metrics import classification_report\n",
    "    import pickle\n",
    "\n",
    "    #First we get two things to compare to\n",
    "    x = df.loc[:, nameOfX]\n",
    "    y = df.loc[:, nameOfY]\n",
    "\n",
    "    #Now we split up the dataset with trainings and testings for both features\n",
    "    x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state = 30)\n",
    "\n",
    "    #This model will be x to y\n",
    "    #Using pipeline from sckit learn to combine data\n",
    "    #We will convert the text to a vector then to a tf-idf for frequency\n",
    "    logesticRegression = Pipeline([('vect', CountVectorizer()), ('tfidf', TfidfTransformer()), ('clf', LogisticRegression(n_jobs=1, C=1e5)),])\n",
    "\n",
    "    #Fit the logesticRegression to our data\n",
    "    logesticRegression.fit(x_train, y_train)\n",
    "\n",
    "    #Predict our data now\n",
    "    y_pred = logesticRegression.predict(x_test)\n",
    "\n",
    "    #Print it all out nice and pretty like :)\n",
    "    print(nameOfX , \"and\", nameOfY)\n",
    "    print('accuracy %s' % accuracy_score(y_pred, y_test))\n",
    "    print(classification_report(y_test, y_pred,target_names=Orders))\n",
    "    \n",
    "\n",
    "#Lets try out the creation!\n",
    "logesticModelerFun(\"University\", \"Order\")\n",
    "logesticModelerFun(\"Major\", \"Order\")\n",
    "logesticModelerFun(\"Year\", \"Order\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'\\x80\\x04\\x95#\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x8c\\x08__main__\\x94\\x8c\\x12logesticModelerFun\\x94\\x93\\x94.'"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Lets pickle the function to end things off\n",
    "import pickle\n",
    "pickledClassification = pickle.dumps(logesticModelerFun)\n",
    "\n",
    "#Show it off serialized \n",
    "pickledClassification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To determine if this is a suitable course of action, I would want to make multiple more classification models to find the one with the highest accuracy. I would also like a slightly larger dataset near around 10,000 entries so that we can maximize the accuracy of our models. I would then like to do a soft deployment where we have people just test it and compare it to the real-world results before we completely started relying on our models, just so if anything goes wrong, we have time to readjust and fix the models. Overall, I think modeling is perfect for this application, it is just finding the correct one to use."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
